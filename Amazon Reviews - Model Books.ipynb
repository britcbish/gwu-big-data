{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Reviews - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.appName(\"amazon-reviews-project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for now, only reading reviews for items in the \"Kitchen\" category\n",
    "reviews = sqlContext.read.parquet(\"s3://amazon-reviews-pds/parquet/product_category=Books/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20726160"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- vine: string (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction\n",
    "Applying a filter - keeping only reviews with higher than 100 total votes received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.filter(reviews.total_votes > 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96785"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining sentiment polarity from review string contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.na.fill({'review_body': '', 'review_headline': ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "from textblob import TextBlob\n",
    "\n",
    "polarity = udf(lambda x: TextBlob(x).sentiment.polarity)\n",
    "reviewLength = udf(lambda x: len(x))\n",
    "\n",
    "reviews = reviews.withColumn('headline_polarity', polarity('review_headline'))\\\n",
    "                 .withColumn('body_polarity', polarity('review_body'))\\\n",
    "                 .withColumn('headline_length', reviewLength('review_headline'))\\\n",
    "                 .withColumn('body_length', reviewLength('review_body'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating \"helpful?\" variable - a review is helpful if at least 75% of 'total_votes' have been 'helpful_votes'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "reviews = reviews.withColumn(\"helpful-ratio\", reviews.helpful_votes/reviews.total_votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.withColumn(\"helpful?\", f.when(reviews[\"helpful-ratio\"] > 0.75, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.withColumn(\"verified_purchase\", f.when(reviews[\"verified_purchase\"] == \"Y\", 1).otherwise(reviews.verified_purchase))\n",
    "reviews = reviews.withColumn(\"verified_purchase\", f.when(reviews[\"verified_purchase\"] == \"N\", 0).otherwise(reviews.verified_purchase))\n",
    "reviews = reviews.withColumn(\"vine\", f.when(reviews[\"vine\"] == \"Y\", 1).otherwise(reviews.vine))\n",
    "reviews = reviews.withColumn(\"vine\", f.when(reviews[\"vine\"] == \"N\", 0).otherwise(reviews.vine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(marketplace='US', customer_id='52472994', review_id='R32S2RSQ6B7CYM', product_id='1937856402', product_parent='394890082', product_title='What Really Happened: John Edwards, Our Daughter, and Me', star_rating=1, helpful_votes=131, total_votes=136, vine='0', verified_purchase='1', review_headline='Thank God for Rielle Hunter', review_body='Why do I say that? If it weren\\'t for Rielle Hunter, we citizens of America may have ended up with John Edwards in high public office, if not the presidency, perhaps a senior role in the administration. What we learn in this book is that John Edwards is a man whose personal maturity was stunted at the age of about 17, a serial liar, and someone who spends 4 hours a day on the phone with his mistress instead of reading public policy papers as he runs for the presidency. So, Rielle helped sweep him from the stage of American political life.  The two teenage narcissists, Rielle and John, clearly deserve each other (although the latest in the soap opera is \\\\\\\\\"they\\'ve broken up\\\\\\\\\"). The picture that emerges of Rielle from her own book is a middle-aged woman with zero professional accomplishments (what a role model for Quinn), the maturity of a teenage girl, who takes large sums of money from other people because she is broke. So many pages of the book are spent relating how she went from city to city, hotel to hotel, meal to meal, friend to friend. It\\'s a life that those of us who really work for a living and who have healthy family relationships wouldn\\'t recognize.  We\\'ve all known a friend like Rielle, who seems to revel in the high drama that she creates in her life, all the while earnestly protesting \\\\\\\\\"this isn\\'t what I want\\\\\\\\\".<br /><br />What\\'s amazing is this:  Despite all of the revelations in the book about Elizabeth Edwards and even if you believe most of them are likely an accurate characterization, I would much rather spend a day with Elizabeth than Rielle.', review_date=datetime.date(2012, 6, 29), year=2012, headline_polarity='0.0', body_polarity='0.15324175824175826', headline_length='27', body_length='1590', helpful-ratio=0.9632352941176471, helpful?=1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.ml.regression as rg\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.ml.feature as feat\n",
    "import pyspark.ml.classification as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running bucketizer for pickup_longitude and adding it in the dataset\n",
    "splits = [-float(\"inf\"), 0, 5, float(\"inf\")]\n",
    "\n",
    "bucketizer = feat.Bucketizer(splits=splits, inputCol=\"year\", outputCol=\"year_bkt\")\n",
    "\n",
    "reviews = bucketizer.transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.drop('customer_id','review_id','product_id','parent_product','product title', 'helpful_votes', 'review_headline', 'review_body', 'review_date', 'year', 'helpful-ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- product_parent: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- vine: string (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- headline_polarity: string (nullable = true)\n",
      " |-- body_polarity: string (nullable = true)\n",
      " |-- headline_length: string (nullable = true)\n",
      " |-- body_length: string (nullable = true)\n",
      " |-- helpful?: integer (nullable = false)\n",
      " |-- year_bkt: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import IntegerType  \n",
    "reviews = reviews.withColumn(\"headline_polarity\", reviews[\"headline_polarity\"].cast(FloatType()))\n",
    "reviews = reviews.withColumn(\"body_polarity\", reviews[\"body_polarity\"].cast(FloatType()))\n",
    "reviews = reviews.withColumn(\"headline_length\", reviews[\"headline_polarity\"].cast(FloatType()))\n",
    "reviews = reviews.withColumn(\"body_length\", reviews[\"body_polarity\"].cast(FloatType()))\n",
    "reviews = reviews.withColumn(\"vine\", reviews[\"vine\"].cast(IntegerType()))\n",
    "reviews = reviews.withColumn(\"verified_purchase\", reviews[\"verified_purchase\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=reviews.drop('features') #removes the column 'features' if it already exists\n",
    "#selects all numeric columns to be combined into column 'features'\n",
    "Cols_to_Select = reviews[\"star_rating\", \"total_votes\", \"headline_polarity\", \"body_polarity\", \"headline_length\", \"body_length\", \"year_bkt\", \"vine\", \"verified_purchase\"]\n",
    "assembler = feat.VectorAssembler(inputCols=Cols_to_Select.columns, outputCol=\"features\") #creates the VectorAssembler object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the VectorAssembler transformation onto the dataframe to create the 'features' column\n",
    "reviews=assembler.setHandleInvalid(\"skip\").transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into train, test, and predict datasets\n",
    "splitted_data = reviews.randomSplit([0.7, 0.3], 199)\n",
    "train_data = splitted_data[0]\n",
    "test_data = splitted_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the logistic regression object \n",
    "logReg_obj = cl.LogisticRegression(\n",
    "    labelCol=\"helpful?\"\n",
    "    , featuresCol = \"features\",\n",
    "    maxIter=5, regParam=0.3, elasticNetParam=0.8\n",
    ")\n",
    "# using pipeline to run the logistic regression, plus all other objects intially created\n",
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        logReg_obj\n",
    "    ])\n",
    "\n",
    "pipelineModel = pipeline.fit(train_data) #running the model on training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC: 0.8429486028016648\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = pipelineModel.stages[-1].summary\n",
    "\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "#evaluating the model created against test dataset\n",
    "results_logReg = (\n",
    "    pipelineModel\n",
    "    .transform(test_data)\n",
    "    .select('helpful?', 'probability', 'prediction')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ev.MulticlassClassificationEvaluator(\n",
    "    predictionCol='prediction'\n",
    "    , labelCol='helpful?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5067435406036527, 0.41698501148555084, 0.645743766122098)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    evaluator.evaluate(results_logReg)\n",
    "    , evaluator.evaluate(\n",
    "        results_logReg\n",
    "        , {evaluator.metricName: 'weightedPrecision'}\n",
    "    ) \n",
    "    , evaluator.evaluate(\n",
    "        results_logReg\n",
    "        , {evaluator.metricName: 'accuracy'}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
